{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hacker News Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this guided project, from a JSON API, we will filter, clean, aggregate, and summarize data in a sequence of tasks that will apply these transformations for us.\n",
    "\n",
    "The data we will use comes from a Hacker News (HN) API that returns JSON data of the top stories in 2014. Hacker News is a link aggregator website that users vote up stories that are interesting to the community. It is similar to Reddit, but the community only revolves around on computer science and entrepreneurship posts.\n",
    "\n",
    "The data used for the project includes a list of JSON posts in a file called *hn_stories_2014.json*. The JSON file contains a single key stories, which contains a list of stories (posts). Each post has a set of keys, but we will deal only with the following keys:\n",
    "\n",
    "- **created_at**: A timestamp of the story's creation time.\n",
    "- **created_at_i**: A unix epoch timestamp.\n",
    "- **url**: The URL of the story link.\n",
    "- **objectID**: The ID of the story.\n",
    "- **author**: The story's author (username on HN).\n",
    "- **points**: The number of upvotes the story had.\n",
    "- **title**: The headline of the post.\n",
    "- **num_comments**: The number of a comments a post has.\n",
    "\n",
    "Using this dataset, we will run a sequence of basic natural language processing tasks using our Pipeline class. The goal will be to find the top 100 keywords of Hacker News posts in 2014. Because Hacker News is the most popular technology social media site, this will give us an understanding of the most talked about tech topics in 2014!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import io\n",
    "import csv\n",
    "import string\n",
    "\n",
    "from pipeline import build_csv, Pipeline\n",
    "from stop_words import stop_words\n",
    "\n",
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a task function that takes no argument\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json', 'r') as f:\n",
    "        data = json.load(f)       # load json file in a dict\n",
    "        stories = data['stories'] # return the list of stories\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the stories\n",
    "\n",
    "Like any social link aggregator site, individual users can post whatever content they want. The reason we want the most popular stories is to ensure that we select stories that were the most talked about during the year. We can filter for popular stories by ensuring they are links (not *Ask HN* posts), have a good number of points, and have some comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a task function that depends on the file_to_json() function\n",
    "@pipeline.task(depends_on=file_to_json)\n",
    "\n",
    "# Filter popular stories that have \n",
    "# more than 50 points, more than 1 comment, and do not begin with Ask HN\n",
    "def filter_stories(stories):\n",
    "    def is_popular(story):\n",
    "        return story['points'] > 50 and story['num_comments'] > 1 and not story['title'].startswith('Ask HN') \n",
    "    # Return a generator of these filtered stories\n",
    "    return (story for story in stories if is_popular(story)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to CSV\n",
    "\n",
    "With a reduced set of stories, it's time to write these dict objects to a CSV file. The purpose of translating the dictionaries to a CSV is that we want to have a consistent data format when running the later summarizations. By keeping consistent data formats, each of your pipeline tasks will be adaptable with future task requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a task function that depends on the filter_stories() function\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "\n",
    "# Create a function to writed filtered JSON stories to a CSV file\n",
    "def json_to_csv(stories): # return value from build_csv using lines, header and io.StringIO() file\n",
    "    lines = []\n",
    "    for story in stories:\n",
    "        lines.append(\n",
    "            (story['objectID'], \n",
    "             datetime.strptime(story['created_at'], \"%Y-%m-%dT%H:%M:%SZ\"), \n",
    "             story['url'], story['points'], story['title'])\n",
    "        )\n",
    "    return build_csv(lines, \n",
    "                     header=['objectID', 'created_at', 'url', 'points', 'title'], \n",
    "                     file=io.StringIO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Title Column\n",
    "\n",
    "Using the CSV file format we created in the previous task, we can now extract the title column. Once we have extracted the titles of each popular post, we can then run the next word frequency task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a task function that depends on the json_to_csv() function\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "\n",
    "# Create a function that returns a generator of every Hacker News story title\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file) # create object from the file object\n",
    "    header = next(reader)         # get header\n",
    "    idx = header.index('title')   # find the index of the title in the header\n",
    "    # Loop through the reader and return each item of the title index position\n",
    "    return (line[idx] for line in reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Titles\n",
    "\n",
    "Because we're trying to create a word frequency model of words from Hacker News titles, we need a way to create a consistent set of words to use. For example, words like *Google, google, GooGle?,* and *google.*, all mean the same keyword: google. If we were to split the title into words, however, they would all be lumped into different categories.\n",
    "\n",
    "To clean the titles, we should make sure to lower case the titles, and to remove the punctuation. An easy way to rid a string of punctuation is to check each character, determine if it is a letter or punctuation, and only keep the letter. From the *string* package, we are given a handy string constant that contains all the punctuation needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a task function that depends on the extract_titles() function\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_title(titles): # return cleaned titles\n",
    "    for title in titles:\n",
    "        title = title.lower() # title is lower case\n",
    "        title = ''.join(c for c in title if c not in string.punctuation) # remove punctuation from titles\n",
    "        yield title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Word Frequency Dictionary\n",
    "\n",
    "With a cleaned title, we can now build the word frequency dictionary. A word frequency dictionary are key value pairs that connects a word to the number of times it is used in a text. Furthermore, to find actual keywords, we should enforce the word frequency dictionary to not include stop words. Stop words are words that occur frequently in language like \"the\", \"or\", etc., and are commonly rejected in keyword searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a task function that depends on the clean_titles() function\n",
    "@pipeline.task(depends_on=clean_title)\n",
    "\n",
    "# Create a function that returns a dit of the word frequency of all titles\n",
    "def build_keyword_dictionary(titles):\n",
    "    word_freq = {}\n",
    "    for title in titles:\n",
    "        for word in title.split(' '):\n",
    "            if word and word not in stop_words: # word frequency should NOT include stop words\n",
    "                if word not in word_freq:\n",
    "                    word_freq[word] = 1\n",
    "                word_freq[word] += 1\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting the Top Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 186), ('google', 168), ('bitcoin', 102), ('open', 93), ('programming', 91), ('web', 89), ('data', 86), ('video', 80), ('python', 76), ('code', 73), ('facebook', 72), ('released', 72), ('using', 71), ('2013', 66), ('javascript', 66), ('free', 65), ('source', 65), ('game', 64), ('internet', 63), ('microsoft', 60), ('c', 60), ('linux', 59), ('app', 58), ('pdf', 56), ('work', 55), ('language', 55), ('software', 53), ('2014', 53), ('startup', 52), ('apple', 51), ('use', 51), ('make', 51), ('time', 49), ('yc', 49), ('security', 49), ('nsa', 46), ('github', 46), ('windows', 45), ('world', 42), ('way', 42), ('like', 42), ('1', 41), ('project', 41), ('computer', 41), ('heartbleed', 41), ('git', 38), ('users', 38), ('dont', 38), ('design', 38), ('ios', 38), ('developer', 37), ('os', 37), ('twitter', 37), ('ceo', 37), ('vs', 37), ('life', 37), ('big', 36), ('day', 36), ('android', 35), ('online', 35), ('years', 34), ('simple', 34), ('court', 34), ('guide', 33), ('learning', 33), ('mt', 33), ('api', 33), ('says', 33), ('apps', 33), ('browser', 33), ('server', 32), ('firefox', 32), ('fast', 32), ('gox', 32), ('problem', 32), ('mozilla', 32), ('engine', 32), ('site', 32), ('introducing', 31), ('amazon', 31), ('year', 31), ('support', 30), ('stop', 30), ('built', 30), ('better', 30), ('million', 30), ('people', 30), ('text', 30), ('3', 29), ('does', 29), ('tech', 29), ('development', 29), ('billion', 28), ('developers', 28), ('just', 28), ('library', 28), ('did', 28), ('website', 28), ('money', 28), ('inside', 28)]\n"
     ]
    }
   ],
   "source": [
    "# Create a task function that depends on the build_keyword_dictionary() function\n",
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "\n",
    "# Create a function that returns a list of top 100 tuples \n",
    "def top_keywords(word_freq):\n",
    "    freq_tuple = [\n",
    "        (word, word_freq[word])\n",
    "        for word in sorted(word_freq, key=word_freq.get, reverse=True)\n",
    "    ]\n",
    "    return freq_tuple[:100]\n",
    "\n",
    "# Run the pipeline\n",
    "ran = pipeline.run()\n",
    "print(ran[top_keywords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The final result yielded some interesting keywords. There were terms like bitcoin (the cryptocurrency), heartbleed (the 2014 hack), and many others. Even though this was a basic natural language processing task, it did provide some interesting insights into conversations from 2014. \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Rewrite the Pipeline class' output to save a file of the output for each task. This will allow us to \"checkpoint\" tasks so we don't have to be run twice.\n",
    "- Use the nltk package for more advanced natural language processing tasks.\n",
    "- Convert to a CSV before filtering, so we can keep all the stories from 2014 in a raw file.\n",
    "- Fetch the data from Hacker News directly from a JSON API (https://hn.algolia.com/api). Instead of reading from the original file, we can perform additional data processing using newer data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
